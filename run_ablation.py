"""
æ¶ˆèå®éªŒè„šæœ¬
å¯¹æ¯”åˆ†æï¼šæ™ºèƒ½è°ƒåº¦ vs éšæœºè°ƒåº¦ã€FastCDC vs å›ºå®šåˆ†å—
"""

import os
import csv
import time
import random
import threading
from pathlib import Path
from typing import Dict, List, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass

from tqdm import tqdm

# å¯¼å…¥ç³»ç»Ÿæ¨¡å—
from client import Client
from edge_core import EdgeProcessor, ChunkStatus, ChunkInfo
from scheduler import SmartScheduler
from cloud_mock import CloudNode, CLOUD_NODES


@dataclass
class ExperimentResult:
    """å®éªŒç»“æœ"""
    exp_name: str           # å®éªŒåç§°
    total_time: float       # æ€»è€—æ—¶ï¼ˆç§’ï¼‰
    avg_latency: float      # å¹³å‡å»¶è¿Ÿï¼ˆç§’ï¼‰
    dedup_ratio: float      # å»é‡ç‡
    total_chunks: int       # æ€»å—æ•°
    uploaded_shards: int    # ä¸Šä¼ åˆ†ç‰‡æ•°
    cloud_distribution: Dict[str, int]  # äº‘èŠ‚ç‚¹åˆ†å¸ƒ


# ============================================================
# æ•°æ®å‡†å¤‡æ¨¡å—
# ============================================================

def generate_synthetic_workload(path: str = "./test_data/synthetic_workload.bin", 
                                 size_mb: float = 5.0) -> bytes:
    """
    ç”Ÿæˆç»“æ„åŒ–æ¨¡æ‹Ÿæ•°æ®ï¼ˆä»£ç +æ–‡æ¡£æ··åˆï¼‰
    
    ç”ŸæˆåŒ…å«è‹±æ–‡æ–‡æœ¬æ®µè½ã€æ¨¡æ‹Ÿä»£ç ç‰‡æ®µå’Œå°‘é‡äºŒè¿›åˆ¶æ•°æ®çš„æ··åˆæ–‡ä»¶ï¼Œ
    æ¨¡æ‹ŸçœŸå®çš„"ä»£ç +æ–‡æ¡£"å·¥ä½œè´Ÿè½½ç»“æ„ã€‚
    
    Args:
        path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        size_mb: ç›®æ ‡æ–‡ä»¶å¤§å°ï¼ˆMBï¼‰
        
    Returns:
        ç”Ÿæˆçš„æ–‡ä»¶å†…å®¹ï¼ˆå­—èŠ‚ï¼‰
    """
    file_path = Path(path)
    
    if file_path.exists():
        print(f"ğŸ“„ è¯»å–å·²æœ‰å·¥ä½œè´Ÿè½½æ–‡ä»¶: {path}")
        with open(file_path, 'rb') as f:
            return f.read()
    
    print(f"ğŸ“ ç”Ÿæˆç»“æ„åŒ–æ¨¡æ‹Ÿå·¥ä½œè´Ÿè½½: {path}")
    file_path.parent.mkdir(parents=True, exist_ok=True)
    
    target_size = int(size_mb * 1024 * 1024)
    
    # ========== æ–‡æ¡£æ®µè½æ¨¡æ¿ ==========
    doc_paragraphs = [
        b"=" * 80 + b"\n" + b"CHAPTER 1: Introduction to Cloud-Edge-End Storage Systems\n" + b"=" * 80 + b"\n\n",
        b"Cloud storage has revolutionized the way we store and manage data. The ability to access files from anywhere has transformed both personal and enterprise computing. " * 10 + b"\n\n",
        b"Edge computing brings computation closer to data sources, reducing latency and bandwidth consumption. This paradigm shift enables real-time processing of IoT data streams. " * 10 + b"\n\n",
        b"-" * 80 + b"\n" + b"Section 1.1: System Architecture Overview\n" + b"-" * 80 + b"\n\n",
        b"Our proposed CE2S (Cloud-Edge-End Storage) system implements a three-tier architecture. The client layer handles encryption, the edge layer performs deduplication, and the cloud layer provides persistent storage. " * 8 + b"\n\n",
        b"=" * 80 + b"\n" + b"CHAPTER 2: Deduplication Algorithms\n" + b"=" * 80 + b"\n\n",
        b"Content-Defined Chunking (CDC) algorithms divide data at content-dependent boundaries. Unlike fixed-size chunking, CDC maintains chunk alignment even when data is inserted or deleted. " * 10 + b"\n\n",
        b"FastCDC is a gear-based CDC algorithm that achieves 10x speedup over traditional Rabin fingerprinting while maintaining comparable deduplication ratios. " * 10 + b"\n\n",
    ]
    
    # ========== ä»£ç ç‰‡æ®µæ¨¡æ¿ ==========
    code_snippets = [
        b'''
# ============================================================
# cloud_storage.py - Cloud Storage Client Implementation
# ============================================================

import hashlib
import requests
from typing import Optional, Dict, List

class CloudStorageClient:
    """Client for interacting with cloud storage services."""
    
    def __init__(self, endpoint: str, api_key: str):
        self.endpoint = endpoint
        self.api_key = api_key
        self.session = requests.Session()
        self.session.headers.update({"Authorization": f"Bearer {api_key}"})
    
    def upload_chunk(self, chunk_id: str, data: bytes) -> Dict:
        """Upload a data chunk to cloud storage."""
        fingerprint = hashlib.sha256(data).hexdigest()
        response = self.session.post(
            f"{self.endpoint}/chunks/{chunk_id}",
            data=data,
            headers={"X-Fingerprint": fingerprint}
        )
        return response.json()
    
    def download_chunk(self, chunk_id: str) -> Optional[bytes]:
        """Download a chunk from cloud storage."""
        response = self.session.get(f"{self.endpoint}/chunks/{chunk_id}")
        if response.status_code == 200:
            return response.content
        return None

''',
        b'''
# ============================================================
# edge_processor.py - Edge Node Processing Module
# ============================================================

import zlib
from dataclasses import dataclass
from enum import Enum
from typing import List, Tuple

class ChunkType(Enum):
    NEW = "new"
    DUPLICATE = "duplicate"

@dataclass
class ProcessedChunk:
    chunk_id: int
    fingerprint: str
    chunk_type: ChunkType
    data: bytes
    compressed_size: int

class EdgeProcessor:
    """Edge node processor for data deduplication and compression."""
    
    def __init__(self, compression_level: int = 6):
        self.compression_level = compression_level
        self.fingerprint_index: Dict[str, int] = {}
    
    def process_chunks(self, chunks: List[bytes]) -> List[ProcessedChunk]:
        """Process a list of data chunks."""
        results = []
        for idx, chunk in enumerate(chunks):
            fingerprint = self._compute_fingerprint(chunk)
            if fingerprint in self.fingerprint_index:
                chunk_type = ChunkType.DUPLICATE
                compressed = b""
            else:
                chunk_type = ChunkType.NEW
                compressed = zlib.compress(chunk, self.compression_level)
                self.fingerprint_index[fingerprint] = idx
            
            results.append(ProcessedChunk(
                chunk_id=idx,
                fingerprint=fingerprint,
                chunk_type=chunk_type,
                data=chunk if chunk_type == ChunkType.NEW else b"",
                compressed_size=len(compressed)
            ))
        return results

''',
        b'''
# ============================================================
# scheduler.py - Intelligent Cloud Node Scheduler
# ============================================================

import random
from typing import List, Tuple, Optional
from dataclasses import dataclass

@dataclass
class CloudNode:
    node_id: str
    latency_ms: float
    cost_per_gb: float
    available_capacity_gb: float

class SmartScheduler:
    """EWMA-based intelligent scheduler for cloud node selection."""
    
    def __init__(self, alpha: float = 0.7):
        self.alpha = alpha
        self.ewma_latency: Dict[str, float] = {}
    
    def update_latency(self, node_id: str, observed_latency: float) -> None:
        """Update EWMA latency prediction for a node."""
        if node_id not in self.ewma_latency:
            self.ewma_latency[node_id] = observed_latency
        else:
            prev = self.ewma_latency[node_id]
            self.ewma_latency[node_id] = self.alpha * observed_latency + (1 - self.alpha) * prev
    
    def select_best_nodes(self, nodes: List[CloudNode], k: int = 3) -> List[CloudNode]:
        """Select top-k nodes based on predicted latency and cost."""
        scores = []
        for node in nodes:
            pred_latency = self.ewma_latency.get(node.node_id, node.latency_ms)
            score = 0.7 * pred_latency + 0.3 * node.cost_per_gb * 100
            scores.append((node, score))
        scores.sort(key=lambda x: x[1])
        return [node for node, _ in scores[:k]]

''',
        b'''
# ============================================================
# config.py - System Configuration
# ============================================================

import os
from pathlib import Path

# Storage paths
STORAGE_ROOT = Path(os.getenv("CE2S_STORAGE_ROOT", "./storage"))
CLOUD_STORAGE_PATH = STORAGE_ROOT / "cloud"
EDGE_CACHE_PATH = STORAGE_ROOT / "edge_cache"
LOG_PATH = STORAGE_ROOT / "logs"

# Chunking parameters
CHUNK_MIN_SIZE = 4 * 1024      # 4 KB
CHUNK_AVG_SIZE = 8 * 1024      # 8 KB
CHUNK_MAX_SIZE = 16 * 1024     # 16 KB

# Erasure coding parameters
EC_DATA_SHARDS = 4
EC_PARITY_SHARDS = 2

# Cloud nodes configuration
CLOUD_NODES = [
    {"id": "aliyun", "endpoint": "https://oss.aliyun.com", "latency": 20, "cost": 0.8},
    {"id": "tencent", "endpoint": "https://cos.tencent.com", "latency": 50, "cost": 0.5},
    {"id": "huawei", "endpoint": "https://obs.huawei.com", "latency": 100, "cost": 0.3},
    {"id": "baidu", "endpoint": "https://bos.baidu.com", "latency": 500, "cost": 0.1},
]

''',
    ]
    
    # ========== ç»„è£…å†…å®¹ ==========
    content = bytearray()
    
    # æ·»åŠ æ–‡ä»¶å¤´
    header = b"""################################################################################
#                                                                              #
#                  CE2S: Cloud-Edge-End Storage System                         #
#                  Synthetic Workload for Ablation Study                       #
#                                                                              #
################################################################################

"""
    content.extend(header)
    
    # äº¤æ›¿æ·»åŠ æ–‡æ¡£å’Œä»£ç ï¼Œæ¨¡æ‹ŸçœŸå®é¡¹ç›®ç»“æ„
    doc_idx = 0
    code_idx = 0
    section = 0
    
    while len(content) < target_size:
        section += 1
        
        # æ·»åŠ æ–‡æ¡£æ®µè½
        content.extend(doc_paragraphs[doc_idx % len(doc_paragraphs)])
        doc_idx += 1
        
        # æ¯éš”å‡ ä¸ªæ–‡æ¡£æ®µè½æ·»åŠ ä»£ç 
        if section % 2 == 0:
            content.extend(b"\n" + b"```python\n")
            content.extend(code_snippets[code_idx % len(code_snippets)])
            content.extend(b"```\n\n")
            code_idx += 1
        
        # å¶å°”æ·»åŠ äºŒè¿›åˆ¶æ•°æ®ï¼ˆæ¨¡æ‹Ÿå›¾ç‰‡/é™„ä»¶ï¼‰
        if section % 5 == 0:
            binary_size = random.randint(2048, 8192)
            content.extend(b"\n[Binary Data Block: " + str(binary_size).encode() + b" bytes]\n")
            content.extend(os.urandom(binary_size))
            content.extend(b"\n[End Binary Block]\n\n")
    
    # æˆªæ–­åˆ°ç›®æ ‡å¤§å°
    content = bytes(content[:target_size])
    
    # ä¿å­˜æ–‡ä»¶
    with open(file_path, 'wb') as f:
        f.write(content)
    
    print(f"   âœ… ç”Ÿæˆå®Œæˆ: {len(content) / 1024 / 1024:.2f} MB")
    return content


def load_real_dataset(folder_path: str = "./real_docs") -> bytes:
    """
    åŠ è½½çœŸå®æ–‡ä»¶å¤¹æ•°æ®
    
    éå†æŒ‡å®šæ–‡ä»¶å¤¹ä¸‹æ‰€æœ‰æ–‡ä»¶ï¼Œå°†å®ƒä»¬è¯»å–å¹¶æ‹¼æ¥æˆä¸€ä¸ªå·¨å¤§çš„ bytearrayã€‚
    
    Args:
        folder_path: æ–‡ä»¶å¤¹è·¯å¾„
        
    Returns:
        æ‹¼æ¥åçš„æ‰€æœ‰æ–‡ä»¶å†…å®¹ï¼ˆå­—èŠ‚ï¼‰
    """
    folder = Path(folder_path)
    
    if not folder.exists():
        print(f"âš ï¸ æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {folder_path}")
        print("   å°†ä½¿ç”¨åˆæˆæ•°æ®æ›¿ä»£")
        return generate_synthetic_workload()
    
    print(f"ğŸ“‚ åŠ è½½çœŸå®æ•°æ®é›†: {folder_path}")
    
    all_data = bytearray()
    file_count = 0
    
    # é€’å½’éå†æ‰€æœ‰æ–‡ä»¶
    for file_path in folder.rglob("*"):
        if file_path.is_file():
            try:
                with open(file_path, 'rb') as f:
                    file_data = f.read()
                    all_data.extend(file_data)
                    file_count += 1
            except (PermissionError, IOError) as e:
                print(f"   âš ï¸ æ— æ³•è¯»å–æ–‡ä»¶: {file_path} ({e})")
    
    total_size_mb = len(all_data) / (1024 * 1024)
    print(f"   âœ… åŠ è½½å®Œæˆ: {file_count} ä¸ªæ–‡ä»¶, æ€»å¤§å°: {total_size_mb:.2f} MB")
    
    if len(all_data) == 0:
        print("   âš ï¸ æ–‡ä»¶å¤¹ä¸ºç©ºï¼Œä½¿ç”¨åˆæˆæ•°æ®æ›¿ä»£")
        return generate_synthetic_workload()
    
    return bytes(all_data)


def create_version_2_data(raw_data: bytes) -> bytes:
    """
    æ¨¡æ‹Ÿç‰ˆæœ¬è¿­ä»£ï¼šåœ¨æ•°æ®ä¸­é—´æ’å…¥æ–°å†…å®¹
    
    æ¨¡æ‹Ÿç”¨æˆ·ç¼–è¾‘åœºæ™¯ï¼šåœ¨æ•°æ®æµçš„æ­£ä¸­é—´æ’å…¥ï¼ˆInsertï¼‰ç›¸å½“äºåŸå¤§å° 10% çš„æ–°éšæœºæ•°æ®ã€‚
    æ³¨æ„ï¼šè¿™æ˜¯æ’å…¥æ“ä½œï¼Œä¼šé€ æˆåç»­æ•°æ®åç§»ï¼Œä¸æ˜¯è¦†ç›–æ“ä½œã€‚
    
    è¿™ç§æ’å…¥æ“ä½œå¯¹äºæµ‹è¯• FastCDC çš„ä¼˜åŠ¿è‡³å…³é‡è¦ï¼š
    - FastCDCï¼ˆå†…å®¹å®šä¹‰åˆ†å—ï¼‰ï¼šæ’å…¥åï¼Œåªæœ‰æ’å…¥ç‚¹é™„è¿‘çš„å—ä¼šæ”¹å˜ï¼Œåç»­å—è¾¹ç•Œè‡ªåŠ¨é‡æ–°å¯¹é½
    - å›ºå®šåˆ†å—ï¼šæ’å…¥åï¼Œæ‰€æœ‰åç»­å—éƒ½ä¼šåç§»ï¼Œå¯¼è‡´å…¨éƒ¨å¤±æ•ˆ
    
    Args:
        raw_data: åŸå§‹æ•°æ®ï¼ˆç‰ˆæœ¬ 1ï¼‰
        
    Returns:
        ä¿®æ”¹åçš„æ•°æ®ï¼ˆç‰ˆæœ¬ 2ï¼‰
    """
    data_len = len(raw_data)
    insert_size = int(data_len * 0.1)  # æ’å…¥ 10% å¤§å°çš„æ–°æ•°æ®
    insert_pos = data_len // 2          # åœ¨æ­£ä¸­é—´æ’å…¥
    
    print(f"ğŸ“ åˆ›å»ºç‰ˆæœ¬ 2 æ•°æ®ï¼ˆæ¨¡æ‹Ÿç¼–è¾‘ï¼‰:")
    print(f"   åŸå§‹å¤§å°: {data_len / 1024 / 1024:.2f} MB")
    print(f"   æ’å…¥ä½ç½®: å­—èŠ‚ {insert_pos} (æ­£ä¸­é—´)")
    print(f"   æ’å…¥å¤§å°: {insert_size} bytes ({insert_size / 1024:.1f} KB)")
    
    # ç”Ÿæˆè¦æ’å…¥çš„æ–°å†…å®¹ï¼ˆæ¨¡æ‹Ÿç”¨æˆ·æ–°å¢çš„ä»£ç /æ–‡æ¡£ï¼‰
    new_content = bytearray()
    
    # æ·»åŠ ä¸€äº›ç»“æ„åŒ–çš„æ’å…¥å†…å®¹
    new_content.extend(b"\n\n")
    new_content.extend(b"=" * 60 + b"\n")
    new_content.extend(b"[NEW SECTION INSERTED - Version 2 Update]\n")
    new_content.extend(b"=" * 60 + b"\n\n")
    new_content.extend(b"This section was added in version 2 of the document.\n" * 20)
    new_content.extend(b"\n# New code added:\n")
    new_content.extend(b"def new_feature():\n")
    new_content.extend(b"    '''Feature added in v2'''\n")
    new_content.extend(b"    pass\n\n")
    
    # å¡«å……å‰©ä½™éƒ¨åˆ†ç”¨éšæœºæ•°æ®
    remaining = insert_size - len(new_content)
    if remaining > 0:
        new_content.extend(os.urandom(remaining))
    
    # æˆªæ–­åˆ°ç›®æ ‡å¤§å°
    new_content = bytes(new_content[:insert_size])
    
    # æ‰§è¡Œæ’å…¥æ“ä½œï¼šå‰åŠéƒ¨åˆ† + æ–°å†…å®¹ + ååŠéƒ¨åˆ†
    version_2_data = raw_data[:insert_pos] + new_content + raw_data[insert_pos:]
    
    print(f"   æ–°ç‰ˆæœ¬å¤§å°: {len(version_2_data) / 1024 / 1024:.2f} MB")
    print(f"   å¤§å°å¢åŠ : +{insert_size / 1024:.1f} KB (+10%)")
    
    return version_2_data


# ============================================================
# ä¿ç•™åŸæœ‰çš„ generate_mock_real_file ä½œä¸ºå…¼å®¹æ€§åˆ«å
# ============================================================

def generate_mock_real_file(path: str = "./test_data/real_sample.pdf") -> bytes:
    """
    ç”Ÿæˆæ¨¡æ‹ŸçœŸå®æ–‡ä»¶
    
    å¦‚æœæ–‡ä»¶å·²å­˜åœ¨åˆ™è¯»å–ï¼Œå¦åˆ™ç”Ÿæˆä¸€ä¸ª 5MB çš„æ¨¡æ‹Ÿæ–‡ä»¶ã€‚
    ä½¿ç”¨é‡å¤æ–‡æœ¬æ®µè½æ¨¡æ‹ŸçœŸå®æ–‡æ¡£ç»“æ„ï¼ˆè€Œéå…¨éšæœºä¹±ç ï¼‰ã€‚
    
    Args:
        path: æ–‡ä»¶è·¯å¾„
        
    Returns:
        æ–‡ä»¶å†…å®¹ï¼ˆå­—èŠ‚ï¼‰
    """
    file_path = Path(path)
    
    if file_path.exists():
        print(f"ğŸ“„ è¯»å–å·²æœ‰æµ‹è¯•æ–‡ä»¶: {path}")
        with open(file_path, 'rb') as f:
            return f.read()
    
    print(f"ğŸ“ ç”Ÿæˆæ¨¡æ‹ŸçœŸå®æ–‡ä»¶: {path}")
    file_path.parent.mkdir(parents=True, exist_ok=True)
    
    # ç›®æ ‡å¤§å°ï¼š5MB
    target_size = 5 * 1024 * 1024
    
    # åˆ›å»ºå¤šä¸ªä¸åŒçš„æ–‡æœ¬æ®µè½ï¼ˆæ¨¡æ‹Ÿæ–‡æ¡£ç»“æ„ï¼‰
    paragraphs = [
        b"=" * 80 + b"\n" + b"CHAPTER 1: Introduction to Cloud-Edge-End Storage Systems\n" + b"=" * 80 + b"\n\n",
        b"Cloud storage has revolutionized the way we store and manage data. " * 20 + b"\n\n",
        b"Edge computing brings computation closer to data sources, reducing latency. " * 20 + b"\n\n",
        b"The integration of cloud and edge creates a powerful hybrid architecture. " * 20 + b"\n\n",
        b"-" * 80 + b"\n" + b"Section 1.1: System Architecture\n" + b"-" * 80 + b"\n\n",
        b"Our proposed CE2S system consists of three layers: client, edge, and cloud. " * 15 + b"\n\n",
        b"Data flows from clients through edge nodes before reaching cloud storage. " * 15 + b"\n\n",
        b"=" * 80 + b"\n" + b"CHAPTER 2: Deduplication and Chunking Algorithms\n" + b"=" * 80 + b"\n\n",
        b"Content-Defined Chunking (CDC) provides better deduplication than fixed-size. " * 20 + b"\n\n",
        b"FastCDC improves chunking speed while maintaining high deduplication ratio. " * 20 + b"\n\n",
        b"-" * 80 + b"\n" + b"Section 2.1: SHA-256 Fingerprinting\n" + b"-" * 80 + b"\n\n",
        b"Each chunk is hashed using SHA-256 to create a unique fingerprint. " * 15 + b"\n\n",
        b"Duplicate chunks are detected by comparing fingerprints in a hash table. " * 15 + b"\n\n",
        b"=" * 80 + b"\n" + b"CHAPTER 3: Intelligent Scheduling\n" + b"=" * 80 + b"\n\n",
        b"EWMA-based latency prediction enables proactive node selection. " * 20 + b"\n\n",
        b"Multi-objective optimization balances QoS and cost requirements. " * 20 + b"\n\n",
        # æ·»åŠ ä¸€äº›éšæœºäºŒè¿›åˆ¶æ•°æ®ï¼ˆæ¨¡æ‹ŸåµŒå…¥çš„å›¾ç‰‡/å›¾è¡¨ï¼‰
        os.urandom(8192),
        b"\n\n" + b"Figure 1: System Architecture Diagram\n\n",
        os.urandom(4096),
        b"\n\n" + b"Table 1: Performance Comparison\n\n",
    ]
    
    # ç»„è£…æ–‡ä»¶å†…å®¹ï¼ˆé‡å¤æ®µè½ä»¥è¾¾åˆ°ç›®æ ‡å¤§å°ï¼‰
    content = bytearray()
    paragraph_idx = 0
    
    while len(content) < target_size:
        content.extend(paragraphs[paragraph_idx % len(paragraphs)])
        paragraph_idx += 1
    
    # æˆªæ–­åˆ°ç›®æ ‡å¤§å°
    content = bytes(content[:target_size])
    
    # ä¿å­˜æ–‡ä»¶
    with open(file_path, 'wb') as f:
        f.write(content)
    
    print(f"   ç”Ÿæˆå®Œæˆ: {len(content) / 1024 / 1024:.2f} MB")
    return content


# ============================================================
# å®éªŒæ ¸å¿ƒè¿è¡Œé€»è¾‘
# ============================================================

def run_experiment_phase(
    phase_name: str,
    raw_data: bytes,
    iterations: int = 1
) -> List[Dict]:
    """
    è¿è¡Œå®éªŒé˜¶æ®µï¼šå¯¹æ¯” Fixed (ä¼ ç»Ÿ) vs Proposed (æœ¬æ–‡æ–¹æ³•)
    
    å®éªŒæµç¨‹ï¼š
    1. ä½¿ç”¨ create_version_2_data ç”Ÿæˆä¿®æ”¹åçš„ data_v2
    2. å¯¹æ¯”ä¸¤ç»„å®éªŒé…ç½®ï¼š
       - Group A (Fixed): use_fastcdc=False, use_smart_scheduler=True
       - Group B (Proposed): use_fastcdc=True, use_smart_scheduler=True
    3. æ¯ç»„å…ˆè·‘ raw_dataï¼ˆé¢„çƒ­/åŸºå‡†ï¼‰ï¼Œå†è·‘ data_v2
    4. è®¡ç®— data_v2 ä¸Šä¼ æ—¶çš„å»é‡ç‡
    
    Args:
        phase_name: å®éªŒé˜¶æ®µåç§°
        raw_data: åŸå§‹æµ‹è¯•æ•°æ®ï¼ˆç‰ˆæœ¬ 1ï¼‰
        iterations: è¿­ä»£æ¬¡æ•°ï¼ˆç”¨äºè®¡ç®—å¹³å‡å€¼ï¼‰
        
    Returns:
        ç»“æœå­—å…¸åˆ—è¡¨ï¼ŒåŒ…å« Phase, Method, Time(s), DedupRatio(%)
    """
    print("\n" + "â•”" + "â•"*58 + "â•—")
    print("â•‘" + f" Experiment Phase: {phase_name} ".center(58) + "â•‘")
    print("â•š" + "â•"*58 + "â•")
    
    # ç”Ÿæˆç‰ˆæœ¬ 2 æ•°æ®ï¼ˆä¸­é—´æ’å…¥ 10%ï¼‰
    print("\nğŸ“ å‡†å¤‡æµ‹è¯•æ•°æ®...")
    data_v2 = create_version_2_data(raw_data)
    
    # å®éªŒé…ç½®
    experiment_configs = [
        {
            "name": "Fixed-size Chunking",
            "short_name": "Fixed",
            "use_fastcdc": False,
            "use_smart_scheduler": True
        },
        {
            "name": "Proposed (FastCDC)",
            "short_name": "Proposed",
            "use_fastcdc": True,
            "use_smart_scheduler": True
        }
    ]
    
    results = []
    
    for config in experiment_configs:
        print(f"\n{'='*60}")
        print(f"ğŸ§ª å®éªŒç»„: {config['name']}")
        print(f"   FastCDC: {config['use_fastcdc']}, SmartScheduler: {config['use_smart_scheduler']}")
        print(f"{'='*60}")
        
        total_time_sum = 0.0
        dedup_ratio_sum = 0.0
        
        for iter_idx in range(iterations):
            if iterations > 1:
                print(f"\n--- è¿­ä»£ {iter_idx + 1}/{iterations} ---")
            
            # ========== ç»„ä»¶åˆå§‹åŒ– ==========
            client = Client(client_id=f"phase_{phase_name}_{config['short_name']}")
            edge = EdgeProcessor(edge_id=f"edge_{config['short_name']}")
            scheduler = SmartScheduler(cloud_nodes=CLOUD_NODES)
            
            # é¢„çƒ­è°ƒåº¦å™¨
            for _ in range(5):
                for node in CLOUD_NODES:
                    fake_latency = random.gauss(node.latency_mean, node.latency_std)
                    scheduler.update_stats(node.cloud_id, max(0, fake_latency))
            
            chunk_mode = 'fastcdc' if config['use_fastcdc'] else 'fixed'
            
            # ========== Phase 1: å¤„ç† raw_dataï¼ˆé¢„çƒ­/åŸºå‡†ï¼‰==========
            print(f"\n[Phase 1] ğŸ“¤ ä¸Šä¼ åŸå§‹æ•°æ® (Version 1)...")
            
            # ç«¯ä¾§åˆ†å— + å—çº§ MLE åŠ å¯†
            # Client è´Ÿè´£åˆ†å—ï¼ˆFastCDC æˆ– Fixedï¼‰å’ŒåŠ å¯†
            # è¿”å›åŠ å¯†å—åˆ—è¡¨ï¼š[{data, size, fingerprint, key}, ...]
            encrypted_chunks_v1 = client.encrypt_data(raw_data, chunk_mode=chunk_mode)
            
            # è¾¹ç¼˜å¤„ç†ï¼ˆå»é‡æ£€æµ‹ + å†—ä½™ç¼–ç ï¼‰
            # EdgeProcessor åªè´Ÿè´£å»é‡ï¼Œä¸å†åˆ†å—
            chunks_v1 = edge.process(encrypted_chunks_v1)
            new_chunks_v1 = [c for c in chunks_v1 if c.status == ChunkStatus.NEW]
            
            print(f"      æ¨¡å¼: {chunk_mode}")
            print(f"      æ€»å—æ•°: {len(chunks_v1)}, æ–°å—: {len(new_chunks_v1)}")
            
            # æ¨¡æ‹Ÿä¸Šä¼ ï¼ˆå¹¶å‘ï¼‰
            upload_time_v1 = _simulate_upload(
                new_chunks_v1, 
                scheduler if config['use_smart_scheduler'] else None,
                desc="      V1ä¸Šä¼ "
            )
            print(f"      V1 ä¸Šä¼ è€—æ—¶: {upload_time_v1:.3f}s")
            
            # ========== Phase 2: å¤„ç† data_v2ï¼ˆæµ‹è¯•å»é‡ï¼‰==========
            print(f"\n[Phase 2] ğŸ“¤ ä¸Šä¼ ä¿®æ”¹åæ•°æ® (Version 2)...")
            
            # é‡ç½®ç»Ÿè®¡ï¼ˆä¿ç•™æŒ‡çº¹è¡¨ï¼‰
            edge.reset_stats()
            
            start_time = time.time()
            
            # ç«¯ä¾§åˆ†å— + å—çº§ MLE åŠ å¯†
            # ç”±äº MLE + å†…å®¹å®šä¹‰åˆ†å—ï¼š
            # - æœªä¿®æ”¹çš„æ˜æ–‡å— â†’ ç›¸åŒå¯†æ–‡å— â†’ è¢« EdgeProcessor å»é‡
            # - ä¿®æ”¹è¿‡çš„æ˜æ–‡å— â†’ ä¸åŒå¯†æ–‡å— â†’ ä½œä¸ºæ–°å—ä¸Šä¼ 
            encrypted_chunks_v2 = client.encrypt_data(data_v2, chunk_mode=chunk_mode)
            
            # è¾¹ç¼˜å¤„ç†ï¼ˆå»é‡æ£€æµ‹ï¼‰
            chunks_v2 = edge.process(encrypted_chunks_v2)
            
            new_chunks_v2 = [c for c in chunks_v2 if c.status == ChunkStatus.NEW]
            ref_chunks_v2 = [c for c in chunks_v2 if c.status == ChunkStatus.REF]
            
            # è®¡ç®—å»é‡ç‡
            dedup_ratio = len(ref_chunks_v2) / len(chunks_v2) if chunks_v2 else 0
            
            print(f"      æ¨¡å¼: {chunk_mode}")
            print(f"      æ€»å—æ•°: {len(chunks_v2)}")
            print(f"      æ–°å—: {len(new_chunks_v2)}, å¼•ç”¨å—: {len(ref_chunks_v2)}")
            print(f"      å»é‡ç‡: {dedup_ratio * 100:.2f}%")
            
            # æ¨¡æ‹Ÿä¸Šä¼ ï¼ˆåªä¸Šä¼ æ–°å—ï¼‰
            upload_time_v2 = _simulate_upload(
                new_chunks_v2,
                scheduler if config['use_smart_scheduler'] else None,
                desc="      V2ä¸Šä¼ "
            )
            
            total_time = time.time() - start_time
            print(f"      V2 ä¸Šä¼ è€—æ—¶: {upload_time_v2:.3f}s")
            print(f"      æ€»å¤„ç†è€—æ—¶: {total_time:.3f}s")
            
            total_time_sum += total_time
            dedup_ratio_sum += dedup_ratio
        
        # è®¡ç®—å¹³å‡å€¼
        avg_time = total_time_sum / iterations
        avg_dedup = dedup_ratio_sum / iterations
        
        # è®°å½•ç»“æœ
        result = {
            "Phase": phase_name,
            "Method": config['name'],
            "Time(s)": round(avg_time, 3),
            "DedupRatio(%)": round(avg_dedup * 100, 2),
            "TotalChunks": len(chunks_v2),
            "NewChunks": len(new_chunks_v2),
            "RefChunks": len(ref_chunks_v2)
        }
        results.append(result)
        
        print(f"\nğŸ“Š {config['name']} ç»“æœ:")
        print(f"   å¹³å‡è€—æ—¶: {avg_time:.3f}s")
        print(f"   å»é‡ç‡: {avg_dedup * 100:.2f}%")
    
    # æ‰“å°å¯¹æ¯”æ€»ç»“
    print("\n" + "="*60)
    print("ğŸ“ˆ Phase ç»“æœå¯¹æ¯”")
    print("="*60)
    print(f"{'Method':<25} {'Time(s)':<12} {'Dedup(%)':<12}")
    print("-"*60)
    for r in results:
        print(f"{r['Method']:<25} {r['Time(s)']:<12.3f} {r['DedupRatio(%)']:<12.2f}")
    
    # è®¡ç®—æå‡
    if len(results) >= 2:
        fixed_result = results[0]
        proposed_result = results[1]
        
        dedup_improvement = proposed_result['DedupRatio(%)'] - fixed_result['DedupRatio(%)']
        print("-"*60)
        print(f"ğŸ¯ FastCDC å»é‡ç‡æå‡: +{dedup_improvement:.2f}%")
    
    return results


def _simulate_upload(
    chunks: List[ChunkInfo],
    scheduler: SmartScheduler = None,
    desc: str = "ä¸Šä¼ "
) -> float:
    """
    æ¨¡æ‹Ÿå¹¶å‘ä¸Šä¼ ï¼ˆå†…éƒ¨è¾…åŠ©å‡½æ•°ï¼‰
    
    Args:
        chunks: éœ€è¦ä¸Šä¼ çš„æ•°æ®å—åˆ—è¡¨
        scheduler: è°ƒåº¦å™¨ï¼ˆNone åˆ™éšæœºé€‰æ‹©ï¼‰
        desc: è¿›åº¦æ¡æè¿°
        
    Returns:
        ä¸Šä¼ è€—æ—¶ï¼ˆç§’ï¼‰
    """
    if not chunks:
        return 0.0
    
    # å‡†å¤‡ä¸Šä¼ ä»»åŠ¡
    upload_tasks = []
    
    if scheduler:
        best_nodes = scheduler.select_best_nodes(k=3)
        best_node_list = [node for node, score in best_nodes]
    else:
        best_node_list = None
    
    task_idx = 0
    for chunk in chunks:
        for shard_idx, shard in enumerate(chunk.shards):
            if best_node_list:
                target_node = best_node_list[task_idx % len(best_node_list)]
            else:
                target_node = random.choice(CLOUD_NODES)
            
            filename = f"{chunk.fingerprint[:16]}_{shard_idx}.shard"
            upload_tasks.append((target_node, shard, filename))
            task_idx += 1
    
    if not upload_tasks:
        return 0.0
    
    # çº¿ç¨‹å®‰å…¨é”
    stats_lock = threading.Lock()
    total_latency = 0.0
    
    def upload_shard(task):
        target_node, shard, filename = task
        result = target_node.upload(shard, filename)
        return result['latency']
    
    start_time = time.time()
    
    # å¹¶å‘ä¸Šä¼ 
    with ThreadPoolExecutor(max_workers=20) as executor:
        futures = {executor.submit(upload_shard, task): task for task in upload_tasks}
        
        with tqdm(total=len(futures), desc=desc, unit="shard", leave=False) as pbar:
            for future in as_completed(futures):
                latency = future.result()
                with stats_lock:
                    total_latency += latency
                pbar.update(1)
    
    return time.time() - start_time


def run_single_experiment(
    exp_name: str,
    use_fastcdc: bool,
    use_smart_scheduler: bool,
    raw_data: bytes
) -> ExperimentResult:
    """
    è¿è¡Œå•æ¬¡æ¶ˆèå®éªŒ
    
    Args:
        exp_name: å®éªŒåç§°
        use_fastcdc: æ˜¯å¦ä½¿ç”¨ FastCDCï¼ˆå¦åˆ™ä½¿ç”¨å›ºå®šåˆ†å—ï¼‰
        use_smart_scheduler: æ˜¯å¦ä½¿ç”¨æ™ºèƒ½è°ƒåº¦ï¼ˆå¦åˆ™éšæœºé€‰æ‹©ï¼‰
        raw_data: åŸå§‹æµ‹è¯•æ•°æ®
        
    Returns:
        å®éªŒç»“æœ
    """
    print(f"\n{'='*60}")
    print(f"ğŸ§ª å®éªŒ: {exp_name}")
    print(f"   FastCDC: {use_fastcdc}, SmartScheduler: {use_smart_scheduler}")
    print(f"{'='*60}")
    
    # ========== ç»„ä»¶åˆå§‹åŒ– ==========
    client = Client(client_id=f"ablation_{exp_name}")
    edge = EdgeProcessor(edge_id=f"edge_{exp_name}")
    scheduler = SmartScheduler(cloud_nodes=CLOUD_NODES)
    
    # é¢„çƒ­è°ƒåº¦å™¨
    for _ in range(5):
        for node in CLOUD_NODES:
            fake_latency = random.gauss(node.latency_mean, node.latency_std)
            scheduler.update_stats(node.cloud_id, max(0, fake_latency))
    
    # ç»Ÿè®¡å˜é‡
    cloud_distribution = {node.cloud_id: 0 for node in CLOUD_NODES}
    total_latency = 0.0
    uploaded_shards = 0
    
    start_time = time.time()
    
    # ========== Step 1: åŠ å¯† ==========
    print("\n[1/3] ğŸ” å®¢æˆ·ç«¯åŠ å¯†...")
    encrypted_data, key, nonce = client.encrypt_data(raw_data)
    
    # ========== Step 2: è¾¹ç¼˜å¤„ç† ==========
    print("[2/3] ğŸ”ª è¾¹ç¼˜å¤„ç†...")
    chunk_mode = 'fastcdc' if use_fastcdc else 'fixed'
    chunks = edge.process(encrypted_data, mode=chunk_mode)
    
    new_chunks = [c for c in chunks if c.status == ChunkStatus.NEW]
    print(f"      æ¨¡å¼: {chunk_mode}, æ€»å—æ•°: {len(chunks)}, æ–°å—: {len(new_chunks)}")
    
    # ========== Step 3: ä¸Šä¼  ==========
    print("[3/3] â˜ï¸  å¹¶å‘ä¸Šä¼ ...")
    
    # å‡†å¤‡ä¸Šä¼ ä»»åŠ¡
    upload_tasks = []
    
    if use_smart_scheduler:
        # æ™ºèƒ½è°ƒåº¦ï¼šé€‰æ‹© Top 3 èŠ‚ç‚¹
        best_nodes = scheduler.select_best_nodes(k=3)
        best_node_list = [node for node, score in best_nodes]
        print(f"      æ™ºèƒ½è°ƒåº¦é€‰æ‹©: {[n.cloud_id for n in best_node_list]}")
    
    task_idx = 0
    for chunk in new_chunks:
        for shard_idx, shard in enumerate(chunk.shards):
            if use_smart_scheduler:
                target_node = best_node_list[task_idx % len(best_node_list)]
            else:
                target_node = random.choice(CLOUD_NODES)
            
            filename = f"{chunk.fingerprint[:16]}_{shard_idx}.shard"
            upload_tasks.append((target_node, shard, filename))
            task_idx += 1
    
    # çº¿ç¨‹å®‰å…¨é”
    stats_lock = threading.Lock()
    
    def upload_shard(task):
        target_node, shard, filename = task
        result = target_node.upload(shard, filename)
        return {
            'cloud_id': target_node.cloud_id,
            'latency': result['latency'],
            'size': len(shard)
        }
    
    # å¹¶å‘ä¸Šä¼ 
    with ThreadPoolExecutor(max_workers=20) as executor:
        futures = {executor.submit(upload_shard, task): task for task in upload_tasks}
        
        with tqdm(total=len(futures), desc="      ä¸Šä¼ è¿›åº¦", unit="shard") as pbar:
            for future in as_completed(futures):
                result = future.result()
                
                with stats_lock:
                    total_latency += result['latency']
                    uploaded_shards += 1
                    cloud_distribution[result['cloud_id']] += 1
                    
                    if use_smart_scheduler:
                        scheduler.update_stats(result['cloud_id'], result['latency'])
                
                pbar.update(1)
    
    upload_time = time.time() - start_time
    
    # ========== å»é‡ç‡æµ‹è¯•ï¼ˆç¬¬äºŒæ¬¡ä¸Šä¼ ï¼‰ ==========
    print("\n[å»é‡æµ‹è¯•] ä¿®æ”¹ 10% æ•°æ®åé‡æ–°å¤„ç†...")
    
    # å¤åˆ¶å¹¶ä¿®æ”¹ä¸­é—´ 10%
    modified_data = bytearray(raw_data)
    data_len = len(modified_data)
    modify_size = int(data_len * 0.1)
    start_pos = (data_len - modify_size) // 2
    modified_data[start_pos:start_pos + modify_size] = os.urandom(modify_size)
    modified_data = bytes(modified_data)
    
    # é‡ç½®è¾¹ç¼˜å¤„ç†å™¨ç»Ÿè®¡ï¼ˆä¿ç•™æŒ‡çº¹è¡¨ï¼‰
    edge.reset_stats()
    
    # å¤„ç†ä¿®æ”¹åçš„æ•°æ®ï¼ˆä¸åŠ å¯†ï¼Œç›´æ¥æµ‹è¯•åˆ†å—å»é‡ï¼‰
    chunks_second = edge.process(modified_data, mode=chunk_mode)
    
    # è®¡ç®—å»é‡ç‡
    ref_chunks = [c for c in chunks_second if c.status == ChunkStatus.REF]
    dedup_ratio = len(ref_chunks) / len(chunks_second) if chunks_second else 0
    
    print(f"      æ€»å—æ•°: {len(chunks_second)}, å¼•ç”¨å—: {len(ref_chunks)}")
    print(f"      å»é‡ç‡: {dedup_ratio * 100:.2f}%")
    
    total_time = time.time() - start_time
    avg_latency = total_latency / uploaded_shards if uploaded_shards > 0 else 0
    
    # åˆ›å»ºç»“æœ
    result = ExperimentResult(
        exp_name=exp_name,
        total_time=total_time,
        avg_latency=avg_latency,
        dedup_ratio=dedup_ratio,
        total_chunks=len(chunks),
        uploaded_shards=uploaded_shards,
        cloud_distribution=cloud_distribution
    )
    
    print(f"\nğŸ“Š {exp_name} ç»“æœ:")
    print(f"   æ€»è€—æ—¶: {total_time:.3f}s")
    print(f"   å¹³å‡å»¶è¿Ÿ: {avg_latency*1000:.2f}ms")
    print(f"   å»é‡ç‡: {dedup_ratio*100:.2f}%")
    print(f"   äº‘èŠ‚ç‚¹åˆ†å¸ƒ: {cloud_distribution}")
    
    return result


def save_results_to_csv(results: List[ExperimentResult], output_path: str = "experiment_results.csv"):
    """
    å°†å®éªŒç»“æœä¿å­˜ä¸º CSV æ–‡ä»¶
    """
    with open(output_path, 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        
        # å†™å…¥è¡¨å¤´
        writer.writerow([
            'Experiment', 'Total Time (s)', 'Avg Latency (ms)', 
            'Dedup Ratio (%)', 'Total Chunks', 'Uploaded Shards',
            'Aliyun', 'Tencent', 'Huawei', 'Baidu'
        ])
        
        # å†™å…¥æ•°æ®
        for r in results:
            writer.writerow([
                r.exp_name,
                f"{r.total_time:.3f}",
                f"{r.avg_latency*1000:.2f}",
                f"{r.dedup_ratio*100:.2f}",
                r.total_chunks,
                r.uploaded_shards,
                r.cloud_distribution.get('aliyun', 0),
                r.cloud_distribution.get('tencent', 0),
                r.cloud_distribution.get('huawei', 0),
                r.cloud_distribution.get('baidu', 0)
            ])
    
    print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜è‡³: {output_path}")


def print_comparison_table(results: List[ExperimentResult]):
    """
    æ‰“å°å¯¹æ¯”åˆ†æè¡¨
    """
    print("\n" + "="*80)
    print("ğŸ“ˆ æ¶ˆèå®éªŒå¯¹æ¯”åˆ†æ")
    print("="*80)
    
    # è¡¨å¤´
    print(f"\n{'Experiment':<25} {'Time(s)':<10} {'Latency(ms)':<12} {'Dedup(%)':<10} {'Shards':<10}")
    print("-" * 80)
    
    # æ•°æ®è¡Œ
    for r in results:
        print(f"{r.exp_name:<25} {r.total_time:<10.3f} {r.avg_latency*1000:<12.2f} "
              f"{r.dedup_ratio*100:<10.2f} {r.uploaded_shards:<10}")
    
    print("-" * 80)
    
    # åˆ†æ
    if len(results) >= 3:
        proposed = results[0]
        wo_scheduler = results[1]
        wo_fastcdc = results[2]
        
        print("\nğŸ” å…³é”®å‘ç°:")
        
        # è°ƒåº¦å™¨æ•ˆæœ
        scheduler_effect = (wo_scheduler.total_time - proposed.total_time) / wo_scheduler.total_time * 100
        print(f"   â€¢ æ™ºèƒ½è°ƒåº¦å™¨: å‡å°‘ {scheduler_effect:.1f}% ä¸Šä¼ è€—æ—¶ "
              f"({wo_scheduler.total_time:.2f}s â†’ {proposed.total_time:.2f}s)")
        
        # FastCDC æ•ˆæœ
        fastcdc_effect = proposed.dedup_ratio - wo_fastcdc.dedup_ratio
        print(f"   â€¢ FastCDC: æå‡ {fastcdc_effect*100:.1f}% å»é‡ç‡ "
              f"({wo_fastcdc.dedup_ratio*100:.1f}% â†’ {proposed.dedup_ratio*100:.1f}%)")


def main():
    """
    ä¸»å‡½æ•°ï¼šè¿è¡Œå®Œæ•´çš„æ¶ˆèå®éªŒ
    
    å®éªŒæµç¨‹ï¼š
    1. Phase 1: Micro-benchmark (åˆæˆæ•°æ®, 5MB)
    2. Phase 2: Macro-benchmark (çœŸå®æ•°æ®, real_docsæ–‡ä»¶å¤¹)
    3. ä¿å­˜æ‰€æœ‰ç»“æœåˆ° CSV
    """
    print("â•”" + "â•"*62 + "â•—")
    print("â•‘" + " CE2S: Cloud-Edge-End Storage System ".center(62) + "â•‘")
    print("â•‘" + " Comprehensive Ablation Study ".center(62) + "â•‘")
    print("â•‘" + " ç»¼åˆæ¶ˆèå®éªŒï¼šFastCDC vs Fixed-size Chunking ".center(50) + "â•‘")
    print("â•š" + "â•"*62 + "â•")
    
    all_results = []
    
    # ============================================================
    # Phase 1: Micro-benchmark (åˆæˆæ•°æ®)
    # ============================================================
    print("\n")
    print("â–ˆ" * 62)
    print("â–ˆ" + " PHASE 1: Micro-benchmark (Synthetic Data) ".center(60) + "â–ˆ")
    print("â–ˆ" * 62)
    
    # ç”Ÿæˆåˆæˆå·¥ä½œè´Ÿè½½
    synthetic_data = generate_synthetic_workload(
        path="./test_data/synthetic.dat",
        size_mb=5.0
    )
    
    # è¿è¡Œå®éªŒ
    phase1_results = run_experiment_phase(
        phase_name="Micro-benchmark",
        raw_data=synthetic_data,
        iterations=1
    )
    all_results.extend(phase1_results)
    
    # ============================================================
    # Phase 2: Macro-benchmark (çœŸå®æ•°æ®)
    # ============================================================
    print("\n")
    print("â–ˆ" * 62)
    print("â–ˆ" + " PHASE 2: Macro-benchmark (Real Data) ".center(60) + "â–ˆ")
    print("â–ˆ" * 62)
    
    real_docs_path = "./real_docs"
    
    if Path(real_docs_path).exists() and any(Path(real_docs_path).iterdir()):
        print(f"\nğŸ“‚ å‘ç°çœŸå®æ•°æ®æ–‡ä»¶å¤¹: {real_docs_path}")
        
        # åŠ è½½çœŸå®æ•°æ®é›†
        real_data = load_real_dataset(real_docs_path)
        
        # è¿è¡Œå®éªŒ
        phase2_results = run_experiment_phase(
            phase_name="Macro-benchmark",
            raw_data=real_data,
            iterations=1
        )
        all_results.extend(phase2_results)
    else:
        print(f"\nâš ï¸ çœŸå®æ•°æ®æ–‡ä»¶å¤¹ä¸å­˜åœ¨æˆ–ä¸ºç©º: {real_docs_path}")
        print("   è·³è¿‡ Macro-benchmark é˜¶æ®µ")
        print("   æç¤ºï¼šå°† PDF/æ–‡æ¡£æ–‡ä»¶æ”¾å…¥ real_docs æ–‡ä»¶å¤¹å³å¯è¿è¡Œæ­¤é˜¶æ®µ")
    
    # ============================================================
    # ä¿å­˜ç»“æœåˆ° CSV
    # ============================================================
    print("\n")
    print("â–ˆ" * 62)
    print("â–ˆ" + " RESULTS SUMMARY ".center(60) + "â–ˆ")
    print("â–ˆ" * 62)
    
    # è®¡ç®—ååé‡å¹¶ä¿å­˜
    save_final_results(all_results, output_path="experiment_results_final.csv")
    
    # æ‰“å°æœ€ç»ˆæ±‡æ€»è¡¨
    print_final_summary(all_results)
    
    print("\n" + "="*62)
    print("âœ… æ‰€æœ‰å®éªŒå®Œæˆ!")
    print("   ç»“æœæ–‡ä»¶: experiment_results_final.csv")
    print("="*62)


def save_final_results(results: List[Dict], output_path: str = "experiment_results_final.csv"):
    """
    ä¿å­˜æœ€ç»ˆå®éªŒç»“æœåˆ° CSV
    
    CSV å­—æ®µï¼šPhase, Method, Time(s), DedupRatio(%), Throughput(MB/s)
    """
    print(f"\nğŸ’¾ ä¿å­˜ç»“æœåˆ°: {output_path}")
    
    with open(output_path, 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        
        # å†™å…¥è¡¨å¤´
        writer.writerow([
            'Phase', 'Method', 'Time(s)', 'DedupRatio(%)', 
            'TotalChunks', 'NewChunks', 'RefChunks', 'Throughput(MB/s)'
        ])
        
        # å†™å…¥æ•°æ®
        for r in results:
            # ä¼°ç®—æ•°æ®å¤§å°ï¼ˆåŸºäºå—æ•°å’Œå¹³å‡å—å¤§å° 8KBï¼‰
            estimated_size_mb = r.get('TotalChunks', 0) * 8 / 1024  # 8KB per chunk
            time_s = r.get('Time(s)', 1)
            throughput = estimated_size_mb / time_s if time_s > 0 else 0
            
            writer.writerow([
                r.get('Phase', ''),
                r.get('Method', ''),
                f"{r.get('Time(s)', 0):.3f}",
                f"{r.get('DedupRatio(%)', 0):.2f}",
                r.get('TotalChunks', 0),
                r.get('NewChunks', 0),
                r.get('RefChunks', 0),
                f"{throughput:.2f}"
            ])
    
    print(f"   âœ… ä¿å­˜æˆåŠŸ")


def print_final_summary(results: List[Dict]):
    """
    æ‰“å°æœ€ç»ˆæ±‡æ€»è¡¨
    """
    print("\n" + "="*80)
    print("ğŸ“Š å®éªŒç»“æœæ±‡æ€»")
    print("="*80)
    
    # è¡¨å¤´
    print(f"\n{'Phase':<18} {'Method':<25} {'Time(s)':<10} {'Dedup(%)':<10} {'Chunks':<8}")
    print("-"*80)
    
    # æŒ‰ Phase åˆ†ç»„æ˜¾ç¤º
    current_phase = None
    for r in results:
        phase = r.get('Phase', '')
        if phase != current_phase:
            if current_phase is not None:
                print("-"*80)
            current_phase = phase
        
        print(f"{phase:<18} {r.get('Method', ''):<25} "
              f"{r.get('Time(s)', 0):<10.3f} "
              f"{r.get('DedupRatio(%)', 0):<10.2f} "
              f"{r.get('TotalChunks', 0):<8}")
    
    print("-"*80)
    
    # å…³é”®å‘ç°åˆ†æ
    print("\nğŸ” å…³é”®å‘ç°:")
    
    # æŒ‰ Phase åˆ†æ
    phases = set(r.get('Phase', '') for r in results)
    
    for phase in phases:
        phase_results = [r for r in results if r.get('Phase', '') == phase]
        
        fixed_result = next((r for r in phase_results if 'Fixed' in r.get('Method', '')), None)
        proposed_result = next((r for r in phase_results if 'Proposed' in r.get('Method', '')), None)
        
        if fixed_result and proposed_result:
            dedup_fixed = fixed_result.get('DedupRatio(%)', 0)
            dedup_proposed = proposed_result.get('DedupRatio(%)', 0)
            improvement = dedup_proposed - dedup_fixed
            
            print(f"\n   [{phase}]")
            print(f"   â€¢ Fixed-size Chunking å»é‡ç‡: {dedup_fixed:.2f}%")
            print(f"   â€¢ FastCDC (Proposed) å»é‡ç‡: {dedup_proposed:.2f}%")
            print(f"   â€¢ FastCDC å»é‡ç‡æå‡: +{improvement:.2f}%")
            
            if improvement > 50:
                print(f"   âœ¨ FastCDC åœ¨ç‰ˆæœ¬è¿­ä»£åœºæ™¯ä¸‹æ˜¾è‘—ä¼˜äºå›ºå®šåˆ†å—!")


if __name__ == "__main__":
    main()
