"""
å®éªŒ 4ï¼šæ•°æ®ç±»å‹å»é‡æ•ˆæœæµ‹è¯•

è¯„ä¼° FastCDC vs Fixed-size Chunking åœ¨ä¸åŒæ•°æ®ç±»å‹ä¸‹çš„å»é‡æ•ˆæœï¼š
- Code: æºä»£ç æ–‡ä»¶ï¼ˆé«˜å¯å‹ç¼©æ€§ã€ç»“æ„åŒ–ï¼‰
- PDF/Documents: æ–‡æ¡£æ–‡ä»¶ï¼ˆæ··åˆå†…å®¹ï¼‰
- Binary: äºŒè¿›åˆ¶æ–‡ä»¶ï¼ˆä½å†—ä½™ï¼‰

å®éªŒè®¾è®¡ï¼š
1. åŠ è½½å„ç±»å‹æ•°æ®é›†
2. ç”Ÿæˆ Version 2ï¼ˆä¸­é—´æ’å…¥ 5% æ–°æ•°æ®ï¼‰
3. å¯¹æ¯”ä¸¤ç§åˆ†å—æ–¹æ³•çš„å»é‡ç‡
"""

import os
import csv
import time
from pathlib import Path
from typing import Dict, List, Tuple, Any

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from client import Client
from edge_core import EdgeProcessor, ChunkStatus


# ========== æ•°æ®é›†è·¯å¾„é…ç½® ==========

DATASET_PATHS = {
    "Code": "test_data/code",
    "PDF": "real_docs",
    "Binary": "test_data/binary",
}


# ========== æ•°æ®åŠ è½½æ¨¡å— ==========

def load_folder_data(folder_path: str, max_size_mb: float = 10.0) -> bytes:
    """
    åŠ è½½æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰æ–‡ä»¶ï¼Œåˆå¹¶ä¸ºå•ä¸ª bytes å¯¹è±¡
    
    Args:
        folder_path: æ–‡ä»¶å¤¹è·¯å¾„
        max_size_mb: æœ€å¤§åŠ è½½å¤§å°ï¼ˆMBï¼‰ï¼Œé˜²æ­¢å†…å­˜æº¢å‡º
        
    Returns:
        åˆå¹¶åçš„æ–‡ä»¶å†…å®¹
    """
    folder = Path(folder_path)
    
    if not folder.exists():
        print(f"   âš ï¸ æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {folder_path}")
        return None
    
    all_data = bytearray()
    file_count = 0
    max_size = int(max_size_mb * 1024 * 1024 * 10)
    
    # é€’å½’éå†æ‰€æœ‰æ–‡ä»¶
    for file_path in sorted(folder.rglob("*")):
        if file_path.is_file():
            try:
                with open(file_path, 'rb') as f:
                    file_data = f.read()
                    
                    # æ£€æŸ¥æ˜¯å¦è¶…è¿‡æœ€å¤§å¤§å°
                    if len(all_data) + len(file_data) > max_size:
                        remaining = max_size - len(all_data)
                        if remaining > 0:
                            all_data.extend(file_data[:remaining])
                        print(f"   âš ï¸ è¾¾åˆ°æœ€å¤§å¤§å°é™åˆ¶ ({max_size_mb} MB)ï¼Œåœæ­¢åŠ è½½")
                        break
                    
                    all_data.extend(file_data)
                    file_count += 1
            except (PermissionError, IOError) as e:
                print(f"   âš ï¸ æ— æ³•è¯»å–æ–‡ä»¶: {file_path} ({e})")
    
    if len(all_data) == 0:
        return None
    
    return bytes(all_data)


def generate_code_dataset(folder_path: str = "test_data/code", 
                          target_size_mb: float = 2.0) -> bytes:
    """
    ç”Ÿæˆä»£ç æ•°æ®é›†
    
    å¦‚æœæ–‡ä»¶å¤¹å­˜åœ¨åˆ™åŠ è½½ï¼Œå¦åˆ™ç”Ÿæˆæ¨¡æ‹Ÿä»£ç æ–‡ä»¶
    """
    folder = Path(folder_path)
    
    # å°è¯•åŠ è½½å·²æœ‰æ•°æ®
    existing_data = load_folder_data(folder_path)
    if existing_data and len(existing_data) > 100 * 1024:  # è‡³å°‘ 100KB
        return existing_data
    
    # ç”Ÿæˆæ¨¡æ‹Ÿä»£ç 
    print(f"   ğŸ“ ç”Ÿæˆæ¨¡æ‹Ÿä»£ç æ•°æ®...")
    folder.mkdir(parents=True, exist_ok=True)
    
    target_size = int(target_size_mb * 1024 * 1024)
    
    # ä»£ç æ¨¡æ¿
    code_templates = [
        b'''"""
Module: data_processor.py
Description: Data processing utilities for CE2S system
"""

import hashlib
import json
from typing import List, Dict, Optional
from dataclasses import dataclass

@dataclass
class DataChunk:
    """Represents a data chunk with metadata."""
    chunk_id: int
    fingerprint: str
    size: int
    data: bytes
    
    def to_dict(self) -> Dict:
        return {
            "chunk_id": self.chunk_id,
            "fingerprint": self.fingerprint,
            "size": self.size
        }

class DataProcessor:
    """Processes data chunks for storage."""
    
    def __init__(self, chunk_size: int = 8192):
        self.chunk_size = chunk_size
        self.processed_count = 0
    
    def process(self, data: bytes) -> List[DataChunk]:
        """Process data into chunks."""
        chunks = []
        offset = 0
        
        while offset < len(data):
            chunk_data = data[offset:offset + self.chunk_size]
            fingerprint = hashlib.sha256(chunk_data).hexdigest()
            
            chunk = DataChunk(
                chunk_id=len(chunks),
                fingerprint=fingerprint,
                size=len(chunk_data),
                data=chunk_data
            )
            chunks.append(chunk)
            offset += self.chunk_size
            self.processed_count += 1
        
        return chunks

''',
        b'''"""
Module: scheduler.py
Description: Intelligent scheduling for cloud node selection
"""

import random
from typing import List, Tuple
from collections import defaultdict

class EWMAPredictor:
    """EWMA-based latency predictor."""
    
    def __init__(self, alpha: float = 0.7):
        self.alpha = alpha
        self.predictions = {}
    
    def update(self, node_id: str, latency: float) -> float:
        if node_id not in self.predictions:
            self.predictions[node_id] = latency
        else:
            self.predictions[node_id] = (
                self.alpha * latency + 
                (1 - self.alpha) * self.predictions[node_id]
            )
        return self.predictions[node_id]
    
    def predict(self, node_id: str, default: float = 100.0) -> float:
        return self.predictions.get(node_id, default)

class SmartScheduler:
    """QoS-Cost aware scheduler."""
    
    def __init__(self, nodes: List[dict], 
                 latency_weight: float = 0.8,
                 cost_weight: float = 0.2):
        self.nodes = nodes
        self.latency_weight = latency_weight
        self.cost_weight = cost_weight
        self.predictor = EWMAPredictor()
    
    def select_nodes(self, k: int = 3) -> List[dict]:
        """Select top-k nodes based on score."""
        scores = []
        for node in self.nodes:
            latency = self.predictor.predict(node["id"], node["latency"])
            cost = node["cost"]
            score = self.latency_weight * latency + self.cost_weight * cost * 100
            scores.append((node, score))
        
        scores.sort(key=lambda x: x[1])
        return [node for node, _ in scores[:k]]

''',
        b'''"""
Module: storage_client.py
Description: Cloud storage client implementation
"""

import os
import time
import hashlib
from pathlib import Path
from typing import Optional, Dict, Any

class StorageClient:
    """Client for cloud storage operations."""
    
    def __init__(self, endpoint: str, api_key: str):
        self.endpoint = endpoint
        self.api_key = api_key
        self.upload_count = 0
        self.download_count = 0
    
    def upload(self, data: bytes, filename: str) -> Dict[str, Any]:
        """Upload data to cloud storage."""
        fingerprint = hashlib.sha256(data).hexdigest()
        
        # Simulate network latency
        time.sleep(0.01)
        
        self.upload_count += 1
        
        return {
            "status": "success",
            "filename": filename,
            "size": len(data),
            "fingerprint": fingerprint,
            "timestamp": time.time()
        }
    
    def download(self, filename: str) -> Optional[bytes]:
        """Download data from cloud storage."""
        # Simulate network latency
        time.sleep(0.01)
        
        self.download_count += 1
        
        # Return mock data
        return b"mock_data"
    
    def get_stats(self) -> Dict[str, int]:
        return {
            "uploads": self.upload_count,
            "downloads": self.download_count
        }

''',
    ]
    
    # ç”Ÿæˆå†…å®¹
    content = bytearray()
    template_idx = 0
    
    while len(content) < target_size:
        content.extend(code_templates[template_idx % len(code_templates)])
        content.extend(b"\n" + b"#" * 80 + b"\n\n")
        template_idx += 1
    
    return bytes(content[:target_size])


def generate_binary_dataset(folder_path: str = "test_data/binary",
                            target_size_mb: float = 2.0) -> bytes:
    """
    ç”ŸæˆäºŒè¿›åˆ¶æ•°æ®é›†
    
    å¦‚æœæ–‡ä»¶å¤¹å­˜åœ¨åˆ™åŠ è½½ï¼Œå¦åˆ™ç”ŸæˆéšæœºäºŒè¿›åˆ¶æ•°æ®
    """
    folder = Path(folder_path)
    
    # å°è¯•åŠ è½½å·²æœ‰æ•°æ®
    existing_data = load_folder_data(folder_path)
    if existing_data and len(existing_data) > 100 * 1024:
        return existing_data
    
    # ç”Ÿæˆæ¨¡æ‹ŸäºŒè¿›åˆ¶æ•°æ®
    print(f"   ğŸ“ ç”Ÿæˆæ¨¡æ‹ŸäºŒè¿›åˆ¶æ•°æ®...")
    folder.mkdir(parents=True, exist_ok=True)
    
    target_size = int(target_size_mb * 1024 * 1024)
    
    # æ··åˆç”Ÿæˆï¼šéƒ¨åˆ†éšæœºï¼Œéƒ¨åˆ†é‡å¤å—ï¼ˆæ¨¡æ‹ŸçœŸå®äºŒè¿›åˆ¶æ–‡ä»¶ï¼‰
    content = bytearray()
    
    # ç”Ÿæˆä¸€äº›å¯èƒ½é‡å¤çš„å—
    repeat_blocks = [os.urandom(8192) for _ in range(10)]
    
    while len(content) < target_size:
        # 70% éšæœºæ•°æ®ï¼Œ30% é‡å¤å—
        if random.random() < 0.7:
            content.extend(os.urandom(random.randint(4096, 16384)))
        else:
            content.extend(random.choice(repeat_blocks))
    
    return bytes(content[:target_size])


def generate_pdf_dataset(folder_path: str = "real_docs",
                         target_size_mb: float = 2.0) -> bytes:
    """
    ç”Ÿæˆæ–‡æ¡£æ•°æ®é›†
    
    å¦‚æœæ–‡ä»¶å¤¹å­˜åœ¨åˆ™åŠ è½½ï¼Œå¦åˆ™ç”Ÿæˆæ¨¡æ‹Ÿæ–‡æ¡£æ•°æ®
    """
    folder = Path(folder_path)
    
    # å°è¯•åŠ è½½å·²æœ‰æ•°æ®
    existing_data = load_folder_data(folder_path)
    if existing_data and len(existing_data) > 100 * 1024:
        return existing_data
    
    # ç”Ÿæˆæ¨¡æ‹Ÿæ–‡æ¡£æ•°æ®
    print(f"   ğŸ“ ç”Ÿæˆæ¨¡æ‹Ÿæ–‡æ¡£æ•°æ®...")
    folder.mkdir(parents=True, exist_ok=True)
    
    target_size = int(target_size_mb * 1024 * 1024)
    
    # æ–‡æ¡£æ®µè½æ¨¡æ¿
    paragraphs = [
        b"=" * 80 + b"\n" + b"CHAPTER 1: Introduction to Cloud Storage\n" + b"=" * 80 + b"\n\n",
        b"Cloud storage has revolutionized the way we store and manage data. The ability to access files from anywhere has transformed both personal and enterprise computing. " * 15 + b"\n\n",
        b"Edge computing brings computation closer to data sources, reducing latency and bandwidth consumption. This paradigm shift enables real-time processing of IoT data streams. " * 15 + b"\n\n",
        b"-" * 80 + b"\n" + b"Section 1.1: System Architecture\n" + b"-" * 80 + b"\n\n",
        b"Our proposed system implements a three-tier architecture. The client layer handles encryption, the edge layer performs deduplication, and the cloud layer provides persistent storage. " * 12 + b"\n\n",
        b"=" * 80 + b"\n" + b"CHAPTER 2: Deduplication Algorithms\n" + b"=" * 80 + b"\n\n",
        b"Content-Defined Chunking (CDC) algorithms divide data at content-dependent boundaries. Unlike fixed-size chunking, CDC maintains chunk alignment even when data is inserted or deleted. " * 15 + b"\n\n",
        b"FastCDC is a gear-based CDC algorithm that achieves significant speedup over traditional Rabin fingerprinting while maintaining comparable deduplication ratios. " * 15 + b"\n\n",
        # æ¨¡æ‹ŸåµŒå…¥çš„å›¾ç‰‡/è¡¨æ ¼
        b"\n[Figure 1: Architecture Diagram]\n" + os.urandom(4096) + b"\n",
        b"\n[Table 1: Performance Comparison]\n" + os.urandom(2048) + b"\n",
    ]
    
    content = bytearray()
    para_idx = 0
    
    while len(content) < target_size:
        content.extend(paragraphs[para_idx % len(paragraphs)])
        para_idx += 1
    
    return bytes(content[:target_size])


# ========== æ•°æ®é›†åŠ è½½å™¨ ==========

def load_dataset(data_type: str) -> bytes:
    """
    æ ¹æ®æ•°æ®ç±»å‹åŠ è½½æ•°æ®é›†
    
    Args:
        data_type: æ•°æ®ç±»å‹ ("Code", "PDF", "Binary")
        
    Returns:
        æ•°æ®é›†å†…å®¹
    """
    folder_path = DATASET_PATHS.get(data_type)
    
    if data_type == "Code":
        return generate_code_dataset(folder_path)
    elif data_type == "PDF":
        return generate_pdf_dataset(folder_path)
    elif data_type == "Binary":
        return generate_binary_dataset(folder_path)
    else:
        raise ValueError(f"Unknown data type: {data_type}")


def create_version_2_data(raw_data: bytes, insert_ratio: float = 0.05) -> bytes:
    """
    æ¨¡æ‹Ÿç‰ˆæœ¬è¿­ä»£ï¼šåœ¨æ•°æ®ä¸­é—´æ’å…¥æ–°å†…å®¹
    
    Args:
        raw_data: åŸå§‹æ•°æ®ï¼ˆç‰ˆæœ¬ 1ï¼‰
        insert_ratio: æ’å…¥æ•°æ®å åŸå§‹æ•°æ®çš„æ¯”ä¾‹ï¼ˆé»˜è®¤ 5%ï¼‰
        
    Returns:
        ä¿®æ”¹åçš„æ•°æ®ï¼ˆç‰ˆæœ¬ 2ï¼‰
    """
    data_len = len(raw_data)
    # åŠ ä¸Šå¥‡æ•°åç§»é‡ (+13 å­—èŠ‚)ï¼Œç¡®ä¿æ’å…¥æ“ä½œæ‰“ä¹±å›ºå®šåˆ†å—çš„è¾¹ç•Œå¯¹é½
    # é˜²æ­¢ insert_size åˆšå¥½æ˜¯åˆ†å—å¤§å°çš„æ•´æ•°å€å¯¼è‡´ Fixed Chunking æ„å¤–å¯¹é½
    insert_size = int(data_len * insert_ratio) + 13
    insert_pos = data_len // 2  # åœ¨æ­£ä¸­é—´æ’å…¥
    
    # ç”Ÿæˆè¦æ’å…¥çš„æ–°å†…å®¹
    new_content = bytearray()
    new_content.extend(b"\n\n[VERSION 2 UPDATE - BEGIN]\n")
    new_content.extend(b"This content was added in version 2.\n" * 10)
    new_content.extend(b"[VERSION 2 UPDATE - END]\n\n")
    
    # å¡«å……å‰©ä½™éƒ¨åˆ†ç”¨éšæœºæ•°æ®
    remaining = insert_size - len(new_content)
    if remaining > 0:
        new_content.extend(os.urandom(remaining))
    
    new_content = bytes(new_content[:insert_size])
    
    # æ‰§è¡Œæ’å…¥æ“ä½œ
    version_2_data = raw_data[:insert_pos] + new_content + raw_data[insert_pos:]
    
    return version_2_data


# ========== å»é‡æµ‹è¯•æ ¸å¿ƒé€»è¾‘ ==========

def run_dedup_test(
    raw_data: bytes,
    data_v2: bytes,
    chunk_mode: str
) -> Tuple[float, int, int, int]:
    """
    è¿è¡Œå»é‡æµ‹è¯•
    
    Args:
        raw_data: åŸå§‹æ•°æ® (V1)
        data_v2: ä¿®æ”¹åæ•°æ® (V2)
        chunk_mode: åˆ†å—æ¨¡å¼ ('fastcdc' æˆ– 'fixed')
        
    Returns:
        (dedup_ratio, total_chunks, new_chunks, ref_chunks)
    """
    # åˆå§‹åŒ–ç»„ä»¶
    client = Client(client_id=f"datatype_test_{chunk_mode}")
    edge = EdgeProcessor(edge_id=f"edge_{chunk_mode}")
    
    # Phase 1: å¤„ç† V1 æ•°æ®ï¼ˆå»ºç«‹æŒ‡çº¹è¡¨ï¼‰
    encrypted_chunks_v1 = client.encrypt_data(raw_data, chunk_mode=chunk_mode)
    chunks_v1 = edge.process(encrypted_chunks_v1)
    
    # Phase 2: å¤„ç† V2 æ•°æ®ï¼ˆæµ‹è¯•å»é‡ï¼‰
    edge.reset_stats()
    encrypted_chunks_v2 = client.encrypt_data(data_v2, chunk_mode=chunk_mode)
    chunks_v2 = edge.process(encrypted_chunks_v2)
    
    # ç»Ÿè®¡
    total_chunks = len(chunks_v2)
    new_chunks = sum(1 for c in chunks_v2 if c.status == ChunkStatus.NEW)
    ref_chunks = sum(1 for c in chunks_v2 if c.status == ChunkStatus.REF)
    
    dedup_ratio = ref_chunks / total_chunks if total_chunks > 0 else 0
    
    return dedup_ratio, total_chunks, new_chunks, ref_chunks


# ========== ä¸»å®éªŒæµç¨‹ ==========

def run_datatype_experiment(
    iterations: int = 3,
    output_dir: str = "results"
) -> List[Dict]:
    """
    è¿è¡Œæ•°æ®ç±»å‹å»é‡æ•ˆæœå®éªŒ
    
    Args:
        iterations: æ¯ä¸ªé…ç½®çš„è¿­ä»£æ¬¡æ•°
        output_dir: ç»“æœè¾“å‡ºç›®å½•
        
    Returns:
        å®éªŒç»“æœåˆ—è¡¨
    """
    print("=" * 70)
    print("  å®éªŒ 4ï¼šæ•°æ®ç±»å‹å»é‡æ•ˆæœæµ‹è¯•")
    print("=" * 70)
    print(f"æ•°æ®ç±»å‹: {list(DATASET_PATHS.keys())}")
    print(f"åˆ†å—æ–¹æ³•: Fixed-size, FastCDC (Proposed)")
    print(f"è¿­ä»£æ¬¡æ•°: {iterations}")
    print("=" * 70)
    
    # åˆ†å—æ–¹æ³•é…ç½®
    methods = [
        ("Fixed-size Chunking", "fixed"),
        ("Proposed (FastCDC)", "fastcdc"),
    ]
    
    results = []
    
    # éå†æ•°æ®ç±»å‹
    for data_type in DATASET_PATHS.keys():
        print(f"\n{'='*70}")
        print(f"ğŸ“ æ•°æ®ç±»å‹: {data_type}")
        print(f"   è·¯å¾„: {DATASET_PATHS[data_type]}")
        print(f"{'='*70}")
        
        # åŠ è½½æ•°æ®é›†
        print(f"\n   ğŸ“‚ åŠ è½½æ•°æ®é›†...")
        raw_data = load_dataset(data_type)
        
        if raw_data is None:
            print(f"   âŒ æ— æ³•åŠ è½½æ•°æ®ï¼Œè·³è¿‡")
            continue
        
        print(f"   âœ… æ•°æ®å¤§å°: {len(raw_data) / 1024:.1f} KB")
        
        # ç”Ÿæˆ V2 æ•°æ®ï¼ˆæ’å…¥ 5% æ–°å†…å®¹ï¼‰
        print(f"\n   ğŸ“ ç”Ÿæˆ Version 2 æ•°æ®ï¼ˆæ’å…¥ 5%ï¼‰...")
        data_v2 = create_version_2_data(raw_data, insert_ratio=0.05)
        print(f"   âœ… V2 æ•°æ®å¤§å°: {len(data_v2) / 1024:.1f} KB (+{(len(data_v2) - len(raw_data)) / 1024:.1f} KB)")
        
        # æµ‹è¯•æ¯ç§åˆ†å—æ–¹æ³•
        for method_name, chunk_mode in methods:
            print(f"\n   ğŸ§ª æµ‹è¯•æ–¹æ³•: {method_name} (mode={chunk_mode})")
            
            dedup_sum = 0.0
            total_chunks_last = 0
            new_chunks_last = 0
            ref_chunks_last = 0
            
            for iter_idx in range(iterations):
                dedup_ratio, total, new, ref = run_dedup_test(
                    raw_data, data_v2, chunk_mode
                )
                
                dedup_sum += dedup_ratio
                total_chunks_last = total
                new_chunks_last = new
                ref_chunks_last = ref
                
                print(f"      è¿­ä»£ {iter_idx+1}: å»é‡ç‡={dedup_ratio*100:.2f}% "
                      f"(æ€»å—={total}, æ–°å—={new}, å¼•ç”¨å—={ref})")
            
            # è®¡ç®—å¹³å‡å»é‡ç‡
            avg_dedup = dedup_sum / iterations
            
            print(f"   âœ… {method_name}: å¹³å‡å»é‡ç‡={avg_dedup*100:.2f}%")
            
            # è®°å½•ç»“æœ
            results.append({
                "DataType": data_type,
                "Method": method_name,
                "DedupRatio(%)": round(avg_dedup * 100, 2),
                "TotalChunks": total_chunks_last,
                "NewChunks": new_chunks_last,
                "RefChunks": ref_chunks_last
            })
    
    # ä¿å­˜ç»“æœ
    Path(output_dir).mkdir(parents=True, exist_ok=True)
    output_file = Path(output_dir) / "datatype_test_results.csv"
    
    with open(output_file, 'w', newline='', encoding='utf-8') as f:
        fieldnames = ["DataType", "Method", "DedupRatio(%)", "TotalChunks", "NewChunks", "RefChunks"]
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(results)
    
    print(f"\n{'='*70}")
    print(f"âœ… å®éªŒå®Œæˆï¼ç»“æœå·²ä¿å­˜è‡³: {output_file}")
    print(f"{'='*70}")
    
    # æ‰“å°ç»“æœæ€»ç»“
    print("\nğŸ“ˆ ç»“æœæ€»ç»“:")
    print(f"{'DataType':<12} {'Method':<25} {'DedupRatio(%)':<15}")
    print("-" * 55)
    for r in results:
        print(f"{r['DataType']:<12} {r['Method']:<25} {r['DedupRatio(%)']:<15.2f}")
    
    # è®¡ç®— FastCDC ç›¸å¯¹äº Fixed çš„æå‡
    print("\nğŸ¯ FastCDC vs Fixed å»é‡ç‡æå‡:")
    for data_type in DATASET_PATHS.keys():
        type_results = [r for r in results if r['DataType'] == data_type]
        if len(type_results) >= 2:
            fixed = next((r for r in type_results if 'Fixed' in r['Method']), None)
            fastcdc = next((r for r in type_results if 'FastCDC' in r['Method']), None)
            
            if fixed and fastcdc:
                improvement = fastcdc['DedupRatio(%)'] - fixed['DedupRatio(%)']
                print(f"   {data_type}: +{improvement:.2f}%")
    
    return results


if __name__ == "__main__":
    import random
    random.seed(42)  # å¯é‡å¤æ€§
    
    # è¿è¡Œå®éªŒ
    results = run_datatype_experiment(
        iterations=3,
        output_dir="results"
    )
